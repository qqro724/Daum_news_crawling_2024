# -*- coding: utf-8 -*-
"""daum_news_crawling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G75DMCoML2f2ni73iMr7AGnuzhpExeXt

###ë‹¤ìŒ ê¸°ì‚¬ ì œëª© í¬ë¡¤ë§
"""

import requests
from bs4 import BeautifulSoup

def get_news_content(url):
    response = requests.get(url)
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # ê¸°ì‚¬ ì œëª© ê°€ì ¸ì˜¤ê¸°
        title_tag = soup.find('strong', class_='tit-g')
        if title_tag:
            title = title_tag.get_text(strip=True)
            print(f"ê¸°ì‚¬ ì œëª©: {title}")

        # ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            print(f"ê¸°ì‚¬ ë‚´ìš©: {content}")
    else:
        print("í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

"""
ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ, íŠ¹ì • ê²€ìƒ‰ì–´ ê²€ìƒ‰ ê²°ê³¼ 50ê°œì˜
URL ì£¼ì†Œ, ê¸°ì‚¬ ì œëª©, ê¸°ì‚¬ ë‚´ìš©ì„ ì¶œë ¥
"""
import requests
from bs4 import BeautifulSoup


def daum_search(keyword):
    url = 'https://search.daum.net/search?w=news&DA=PGD&spacing=0'
    # ê²€ìƒ‰ ê²°ê³¼ëŠ” 1í˜ì´ì§€ë¶€í„° 10í˜ì´ì§€ ê¹Œì§€
    for page in range(1, 11):
        print(f'=== Page {page} ===')
        req_params = {
            'q': keyword, # ê²€ìƒ‰ì–´(í‚¤ì›Œë“œ)ë¥¼ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— íŒŒë¼ë¯¸í„°ë¡œ ì¶”ê°€
            'p': page # ê²€ìƒ‰ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— íŒŒë¼ë¯¸í„°ë¡œ ì¶”ê°€
        }
        response = requests.get(url, params=req_params)
        html = response.text.strip()

        soup = BeautifulSoup(html, 'html.parser')
        news_links = soup.select('.item-title') # css ì„ íƒìëŒ€ë¡œ .class, #id,
        # tag ì™€ ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„
        for link in news_links:
            news_title = link.text
            news_url = link.get('href')
            print(news_url, news_title)


if __name__ == '__main__':
    daum_search('ì´‰ë²•ì†Œë…„')

"""### ê¸°ì‚¬ ë§í¬ / ê¸°ì‚¬ ì œëª© /  ê¸°ì‚¬ ë‚ ì§œ"""

import requests
from bs4 import BeautifulSoup

def get_news_content(url):
    response = requests.get(url)
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # ê¸°ì‚¬ ì œëª© ê°€ì ¸ì˜¤ê¸°
        title_tag = soup.find('strong', class_='tit-g')
        if title_tag:
            title = title_tag.get_text(strip=True)
            print(f"ê¸°ì‚¬ ì œëª©: {title}")

        # ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            print(f"ê¸°ì‚¬ ë‚´ìš©: {content}")
    else:
        print("í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

"""
ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ, íŠ¹ì • ê²€ìƒ‰ì–´ ê²€ìƒ‰ ê²°ê³¼ 50ê°œì˜
URL ì£¼ì†Œ, ê¸°ì‚¬ ì œëª©, ê¸°ì‚¬ ë‚´ìš©, ê¸°ì‚¬ ë‚ ì§œ, ì¸ë„¤ì¼ ì£¼ì†Œë¥¼ ì¶œë ¥
"""
import requests
from bs4 import BeautifulSoup


def daum_search(keyword):
    url = 'https://search.daum.net/search?w=news&DA=PGD&spacing=0'
    # ê²€ìƒ‰ ê²°ê³¼ëŠ” 1í˜ì´ì§€ë¶€í„° 10í˜ì´ì§€ ê¹Œì§€
    for page in range(1, 11):
        print(f'=== Page {page} ===')
        req_params = {
            'q': keyword, # ê²€ìƒ‰ì–´(í‚¤ì›Œë“œ)ë¥¼ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— íŒŒë¼ë¯¸í„°ë¡œ ì¶”ê°€
            'p': page # ê²€ìƒ‰ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— íŒŒë¼ë¯¸í„°ë¡œ ì¶”ê°€
        }
        response = requests.get(url, params=req_params)
        html = response.text.strip()

        soup = BeautifulSoup(html, 'html.parser')
        news_items = soup.select('.item-title') # css ì„ íƒìëŒ€ë¡œ .class, #id,
        # tag ì™€ ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„
        for item in news_items:
            news_title = item.text
            news_url = item.find('a').get('href')
            print(news_url, news_title)

            # ì¶”ê°€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            get_additional_info(news_url)


def get_additional_info(url):
    response = requests.get(url)
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # ì–¸ë¡ ì‚¬ ê°€ì ¸ì˜¤ê¸°
        press_tag = soup.find('span', class_='info_cp')
        if press_tag:
            press = press_tag.get_text(strip=True)
            print(f"ì–¸ë¡ ì‚¬: {press}")

        # ê¸°ì‚¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
        date_tag = soup.find('span', class_='num_date')
        if date_tag:
            date = date_tag.get_text(strip=True)
            print(f"ê¸°ì‚¬ ë‚ ì§œ: {date}")

        # ì¸ë„¤ì¼ ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸°
        thumbnail_tag = soup.find('img', class_='thumb_g')
        if thumbnail_tag:
            thumbnail_url = thumbnail_tag.get('src')
            print(f"ì¸ë„¤ì¼ ì£¼ì†Œ: {thumbnail_url}")
    else:
        print("í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    daum_search('ì´‰ë²•ì†Œë…„')

"""+ ###ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§"""

import requests
from bs4 import BeautifulSoup

def get_news_content(url):
    response = requests.get(url)
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # ê¸°ì‚¬ ì œëª© ê°€ì ¸ì˜¤ê¸°
        title_tag = soup.find('strong', class_='tit-g')
        if title_tag:
            title = title_tag.get_text(strip=True)
            print(f"ê¸°ì‚¬ ì œëª©: {title}")

        # ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            print(f"ê¸°ì‚¬ ë‚´ìš©: {content}")
    else:
        print("í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

"""
ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ, íŠ¹ì • ê²€ìƒ‰ì–´ ê²€ìƒ‰ ê²°ê³¼ 50ê°œì˜
URL ì£¼ì†Œ, ê¸°ì‚¬ ì œëª©, ê¸°ì‚¬ ë‚´ìš©, ê¸°ì‚¬ ë‚ ì§œ, ì¸ë„¤ì¼ ì£¼ì†Œë¥¼ ì¶œë ¥
"""
import requests
from bs4 import BeautifulSoup


def daum_search(keyword):
    url = 'https://search.daum.net/search?w=news&DA=PGD&spacing=0'
    # ê²€ìƒ‰ ê²°ê³¼ëŠ” 1í˜ì´ì§€ë¶€í„° 10í˜ì´ì§€ ê¹Œì§€
    for page in range(1, 11):
        print(f'=== Page {page} ===')
        req_params = {
            'q': keyword, # ê²€ìƒ‰ì–´(í‚¤ì›Œë“œ)ë¥¼ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— íŒŒë¼ë¯¸í„°ë¡œ ì¶”ê°€
            'p': page # ê²€ìƒ‰ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— íŒŒë¼ë¯¸í„°ë¡œ ì¶”ê°€
        }
        response = requests.get(url, params=req_params)
        html = response.text.strip()

        soup = BeautifulSoup(html, 'html.parser')
        news_items = soup.select('.item-title') # css ì„ íƒìëŒ€ë¡œ .class, #id,
        # tag ì™€ ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„
        for item in news_items:
            news_title = item.text
            news_url = item.find('a').get('href')
            print(news_url, news_title)

            # ì¶”ê°€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            get_additional_info(news_url)


def get_additional_info(url):
    response = requests.get(url)
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # ì–¸ë¡ ì‚¬ ê°€ì ¸ì˜¤ê¸°
        press_tag = soup.find('span', class_='info_cp')
        if press_tag:
            press = press_tag.get_text(strip=True)
            print(f"ì–¸ë¡ ì‚¬: {press}")

        # ê¸°ì‚¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
        date_tag = soup.find('span', class_='num_date')
        if date_tag:
            date = date_tag.get_text(strip=True)
            print(f"ê¸°ì‚¬ ë‚ ì§œ: {date}")

        # ì¸ë„¤ì¼ ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸°
        thumbnail_tag = soup.find('img', class_='thumb_g')
        if thumbnail_tag:
            thumbnail_url = thumbnail_tag.get('src')
            print(f"ì¸ë„¤ì¼ ì£¼ì†Œ: {thumbnail_url}")

        # ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            print(f"ê¸°ì‚¬ ë‚´ìš©: {content}")
    else:
        print("í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


if __name__ == '__main__':
    daum_search('ì´‰ë²•ì†Œë…„')

import csv
import requests
from bs4 import BeautifulSoup


def daum_search(keyword):
    url = 'https://search.daum.net/search?w=news&DA=PGD&spacing=0'
    # Create a list to hold all the extracted data
    data_list = []
    # ê²€ìƒ‰ ê²°ê³¼ëŠ” 1í˜ì´ì§€ë¶€í„° 2í˜ì´ì§€ê¹Œì§€
    for page in range(1, 3):
        print(f'=== í˜ì´ì§€ {page} ===')
        req_params = {
            'q': keyword,  # ê²€ìƒ‰ì–´(í‚¤ì›Œë“œ)ë¥¼ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— ì¶”ê°€
            'p': page  # ê²€ìƒ‰ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— ì¶”ê°€
        }
        response = requests.get(url, params=req_params)
        html = response.text.strip()

        soup = BeautifulSoup(html, 'html.parser')
        news_items = soup.select('.item-title')  # CSS ì„ íƒìë¡œ .item-title í´ë˜ìŠ¤ë¥¼ ì„ íƒ
        # ê° ë‰´ìŠ¤ í•­ëª©ì— ëŒ€í•´ ì²˜ë¦¬
        for item in news_items:
            news_title = item.text.strip()
            news_url = item.find('a').get('href')
            print(news_url, news_title)

            # ì¶”ê°€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            additional_info = get_additional_info(news_url)
            data_list.append([news_title, additional_info.get('press', 'ì•Œ ìˆ˜ ì—†ìŒ'), additional_info.get('date', 'ì•Œ ìˆ˜ ì—†ìŒ'), additional_info.get('thumbnail_url', 'ì•Œ ìˆ˜ ì—†ìŒ'), additional_info.get('content', 'ì•Œ ìˆ˜ ì—†ìŒ'), additional_info.get('reviews', {'ì¶”ì²œí•´ìš”': 0, 'ì¢‹ì•„ìš”': 0, 'ê°ë™ì´ì—ìš”': 0, 'í™”ë‚˜ìš”': 0, 'ìŠ¬í¼ìš”': 0})])

    # CSV íŒŒì¼ì— ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ì”ë‹ˆë‹¤.
    with open('news_data.csv', 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        # í—¤ë”ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
        writer.writerow(['ì œëª©', 'ì–¸ë¡ ì‚¬', 'ë‚ ì§œ', 'ì¸ë„¤ì¼ URL', 'ë‚´ìš©', 'ë¦¬ë·° ìˆ˜'])
        # ë°ì´í„°ë¥¼ ì”ë‹ˆë‹¤.
        for data_row in data_list:
            # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
            reviews_str = str(data_row[5])
            # CSVì— ë°ì´í„°ë¥¼ ì”ë‹ˆë‹¤.
            writer.writerow(data_row[:5] + [reviews_str])


def get_additional_info(url):
    response = requests.get(url)
    additional_info = {'press': None, 'date': None, 'thumbnail_url': None, 'content': None, 'reviews': None}
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # ì–¸ë¡ ì‚¬ ê°€ì ¸ì˜¤ê¸°
        press_tag = soup.find('a', id='kakaoServiceLogo')
        if press_tag:
            additional_info['press'] = press_tag.text.strip()

        # ê¸°ì‚¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
        date_tag = soup.find('span', class_='num_date')
        if date_tag:
            additional_info['date'] = date_tag.get_text(strip=True)

        # ì¸ë„¤ì¼ ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸°
        thumbnail_tag = soup.find('a', class_='thumb_bf')
        if thumbnail_tag:
            img_tag = thumbnail_tag.find('img')
            if img_tag:
                additional_info['thumbnail_url'] = img_tag.get('src')

        # ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            additional_info['content'] = content

        # ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
        reaction_div = soup.find('div', class_='jsx-637502918 ğŸ¬_selections')
        reaction_counts = {}
        if reaction_div:
            for button in reaction_div.find_all('button'):
                label = button.find('div', class_='jsx-2157231875 ğŸ¬_selection_label').text.strip()
                count = int(button.find('span', class_='jsx-2157231875 ğŸ¬_count_label').text.strip())
                reaction_counts[label] = count
        additional_info['reviews'] = reaction_counts
        return additional_info

if __name__ == '__main__':
    daum_search('ì´‰ë²•ì†Œë…„')

"""### ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°(ë¯¸ì™„)"""

import csv
import requests
from bs4 import BeautifulSoup


def daum_search(keyword):
    url = 'https://search.daum.net/search?w=news&DA=PGD&spacing=0'
    # Create a list to hold all the extracted data
    data_list = []
    # ê²€ìƒ‰ ê²°ê³¼ëŠ” 1í˜ì´ì§€ë¶€í„° 2í˜ì´ì§€ê¹Œì§€
    for page in range(1, 3):
        print(f'=== í˜ì´ì§€ {page} ===')
        req_params = {
            'q': keyword,  # ê²€ìƒ‰ì–´(í‚¤ì›Œë“œ)ë¥¼ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— ì¶”ê°€
            'p': page  # ê²€ìƒ‰ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— ì¶”ê°€
        }
        response = requests.get(url, params=req_params)
        html = response.text.strip()

        soup = BeautifulSoup(html, 'html.parser')
        news_items = soup.select('.item-title')  # CSS ì„ íƒìë¡œ .item-title í´ë˜ìŠ¤ë¥¼ ì„ íƒ
        # ê° ë‰´ìŠ¤ í•­ëª©ì— ëŒ€í•´ ì²˜ë¦¬
        for item in news_items:
            news_title = item.text.strip()
            news_url = item.find('a').get('href')
            print(news_url, news_title)

            # ì¶”ê°€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            additional_info = get_additional_info(news_url)
            data_list.append([news_title, additional_info.get('press', 'ì•Œ ìˆ˜ ì—†ìŒ'), additional_info.get('date', 'ì•Œ ìˆ˜ ì—†ìŒ'), additional_info.get('thumbnail_url', 'ì•Œ ìˆ˜ ì—†ìŒ'), additional_info.get('content', 'ì•Œ ìˆ˜ ì—†ìŒ'), additional_info.get('reviews', {'ì¶”ì²œí•´ìš”': 0, 'ì¢‹ì•„ìš”': 0, 'ê°ë™ì´ì—ìš”': 0, 'í™”ë‚˜ìš”': 0, 'ìŠ¬í¼ìš”': 0})])

    # CSV íŒŒì¼ì— ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ì”ë‹ˆë‹¤.
    with open('news_data.csv', 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        # í—¤ë”ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
        writer.writerow(['ì œëª©', 'ì–¸ë¡ ì‚¬', 'ë‚ ì§œ', 'ì¸ë„¤ì¼ URL', 'ë‚´ìš©', 'ë¦¬ë·° ìˆ˜'])
        # ë°ì´í„°ë¥¼ ì”ë‹ˆë‹¤.
        for data_row in data_list:
            # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
            reviews_str = str(data_row[5])
            # CSVì— ë°ì´í„°ë¥¼ ì”ë‹ˆë‹¤.
            writer.writerow(data_row[:5] + [reviews_str])


def get_additional_info(url):
    response = requests.get(url)
    additional_info = {'press': None, 'date': None, 'thumbnail_url': None, 'content': None, 'reviews': None}
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # ì–¸ë¡ ì‚¬ ê°€ì ¸ì˜¤ê¸°
        press_tag = soup.find('a', id='kakaoServiceLogo')
        if press_tag:
            additional_info['press'] = press_tag.text.strip()

        # ê¸°ì‚¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
        date_tag = soup.find('span', class_='num_date')
        if date_tag:
            additional_info['date'] = date_tag.get_text(strip=True)

        # ì¸ë„¤ì¼ ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸°
        thumbnail_tag = soup.find('a', class_='thumb_bf')
        if thumbnail_tag:
            img_tag = thumbnail_tag.find('img')
            if img_tag:
                additional_info['thumbnail_url'] = img_tag.get('src')

        # ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            additional_info['content'] = content

        # ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
        reaction_div = soup.find('div', class_='emotion_list')
        reaction_counts = {}
        if reaction_div:
            review_buttons = reaction_div.find_all('button', class_='jsx-2157231875')
            for button in review_buttons:
                label = button.find('div', class_='jsx-2157231875 ğŸ¬_selection_label').text.strip()
                count = button.find('span', class_='jsx-2157231875 ğŸ¬_count_label').text.strip()
                reaction_counts[label] = int(count)
            additional_info['reviews'] = reaction_counts if reaction_counts else None  # ë¦¬ë·°ê°€ ì—†ëŠ” ê²½ìš° Noneìœ¼ë¡œ ì²˜ë¦¬

    return additional_info

if __name__ == '__main__':
    daum_search('ì´‰ë²•ì†Œë…„')

import csv
import requests
from bs4 import BeautifulSoup


def daum_search(keyword):
    url = 'https://search.daum.net/search?w=news&DA=PGD&spacing=0'
    # Create a list to hold all the extracted data
    data_list = []
    # ê²€ìƒ‰ ê²°ê³¼ëŠ” 1í˜ì´ì§€ë¶€í„° 2í˜ì´ì§€ê¹Œì§€
    for page in range(1, 3):
        print(f'=== í˜ì´ì§€ {page} ===')
        req_params = {
            'q': keyword,  # ê²€ìƒ‰ì–´(í‚¤ì›Œë“œ)ë¥¼ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— ì¶”ê°€
            'p': page  # ê²€ìƒ‰ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— ì¶”ê°€
        }
        response = requests.get(url, params=req_params)
        html = response.text.strip()

        soup = BeautifulSoup(html, 'html.parser')
        news_items = soup.select('.item-title')  # CSS ì„ íƒìë¡œ .item-title í´ë˜ìŠ¤ë¥¼ ì„ íƒ
        # ê° ë‰´ìŠ¤ í•­ëª©ì— ëŒ€í•´ ì²˜ë¦¬
        for item in news_items:
            news_title = item.text.strip()
            news_url = item.find('a').get('href')
            print(news_url, news_title)

            # ì¶”ê°€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            additional_info = get_additional_info(news_url)
            data_list.append([news_title, additional_info.get('press', 'ì•Œ ìˆ˜ ì—†ìŒ'), additional_info.get('date', 'ì•Œ ìˆ˜ ì—†ìŒ'), additional_info.get('thumbnail_url', 'ì•Œ ìˆ˜ ì—†ìŒ'), additional_info.get('content', 'ì•Œ ìˆ˜ ì—†ìŒ'), additional_info.get('reviews', {'ì¶”ì²œí•´ìš”': 0, 'ì¢‹ì•„ìš”': 0, 'ê°ë™ì´ì—ìš”': 0, 'í™”ë‚˜ìš”': 0, 'ìŠ¬í¼ìš”': 0})])

    # CSV íŒŒì¼ì— ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ì”ë‹ˆë‹¤.
    with open('news_data.csv', 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        # í—¤ë”ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
        writer.writerow(['ì œëª©', 'ì–¸ë¡ ì‚¬', 'ë‚ ì§œ', 'ì¸ë„¤ì¼ URL', 'ë‚´ìš©', 'ë¦¬ë·° ìˆ˜'])
        # ë°ì´í„°ë¥¼ ì”ë‹ˆë‹¤.
        for data_row in data_list:
            # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
            reviews_str = str(data_row[5])
            # CSVì— ë°ì´í„°ë¥¼ ì”ë‹ˆë‹¤.
            writer.writerow(data_row[:5] + [reviews_str])


def get_additional_info(url):
    response = requests.get(url)
    additional_info = {'press': None, 'date': None, 'thumbnail_url': None, 'content': None, 'reviews': None}
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # ì–¸ë¡ ì‚¬ ê°€ì ¸ì˜¤ê¸°
        press_tag = soup.find('a', id='kakaoServiceLogo')
        if press_tag:
            additional_info['press'] = press_tag.text.strip()

        # ê¸°ì‚¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
        date_tag = soup.find('span', class_='num_date')
        if date_tag:
            additional_info['date'] = date_tag.get_text(strip=True)

        # ì¸ë„¤ì¼ ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸°
        thumbnail_tag = soup.find('a', class_='thumb_bf')
        if thumbnail_tag:
            img_tag = thumbnail_tag.find('img')
            if img_tag:
                additional_info['thumbnail_url'] = img_tag.get('src')

        # ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            additional_info['content'] = content

        # ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
        reaction_div = soup.find('div', class_='emotion_list')
        reaction_counts = {}
        if reaction_div:
            review_labels = reaction_div.find_all('div', class_='ğŸ¬_selection_label')
            review_counts = reaction_div.find_all('span', class_='ğŸ¬_count_label')
            for label, count in zip(review_labels, review_counts):
                reaction_counts[label.text.strip()] = int(count.text.strip())
            additional_info['reviews'] = reaction_counts if reaction_counts else None  # ë¦¬ë·°ê°€ ì—†ëŠ” ê²½ìš° Noneìœ¼ë¡œ ì²˜ë¦¬

    return additional_info

if __name__ == '__main__':
    daum_search('ì´‰ë²•ì†Œë…„')

import pandas as pd

pd.read_csv('/content/news_data.csv')

from bs4 import BeautifulSoup

html = '''
<div data-tiara-custom="alex.component=REACTION&amp;alex.clientId=26BXAvKny5WF5Z09lr5k77Y8&amp;alex.itemKey=20231228152924676&amp;alex.forumKey=news" class="jsx-3762130795 ğŸ¬_main">
    <div class="jsx-637502918 ğŸ¬_selections">
        <button data-action-type="RECOMMEND" data-tiara-action-name="ì•¡ì…˜_ì¶”ì²œí•´ìš”" data-tiara-custom="alex.topType=ì¢‹ì•„ìš”" class="jsx-2157231875 ğŸ¬_selection ğŸ¬_zero_count">
            <div data-animation="SLIDE" class="jsx-2717597395 ğŸ¬_icon"></div>
            <div class="jsx-2157231875 ğŸ¬_selection_label">ì¶”ì²œí•´ìš”</div>
            <span class="jsx-2157231875 ğŸ¬_count_label">3</span>
        </button>
        <button data-action-type="LIKE" data-tiara-action-name="ì•¡ì…˜_ì¢‹ì•„ìš”" data-tiara-custom="alex.topType=ì¢‹ì•„ìš”" class="jsx-2157231875 ğŸ¬_selection">
            <div data-animation="SLIDE" class="jsx-3429346540 ğŸ¬_icon"></div>
            <div class="jsx-2157231875 ğŸ¬_selection_label">ì¢‹ì•„ìš”</div>
            <span class="jsx-2157231875 ğŸ¬_count_label">5</span>
        </button>
        <button data-action-type="IMPRESS" data-tiara-action-name="ì•¡ì…˜_ê°ë™ì´ì—ìš”" data-tiara-custom="alex.topType=ì¢‹ì•„ìš”" class="jsx-2157231875 ğŸ¬_selection ğŸ¬_zero_count">
            <div data-animation="SLIDE" class="jsx-1617167524 ğŸ¬_icon"></div>
            <div class="jsx-2157231875 ğŸ¬_selection_label">ê°ë™ì´ì—ìš”</div>
            <span class="jsx-2157231875 ğŸ¬_count_label">2</span>
        </button>
        <button data-action-type="ANGRY" data-tiara-action-name="ì•¡ì…˜_í™”ë‚˜ìš”" data-tiara-custom="alex.topType=ì¢‹ì•„ìš”" class="jsx-2157231875 ğŸ¬_selection ğŸ¬_zero_count">
            <div data-animation="SLIDE" class="jsx-1027214948 ğŸ¬_icon"></div>
            <div class="jsx-2157231875 ğŸ¬_selection_label">í™”ë‚˜ìš”</div>
            <span class="jsx-2157231875 ğŸ¬_count_label">0</span>
        </button>
        <button data-action-type="SAD" data-tiara-action-name="ì•¡ì…˜_ìŠ¬í¼ìš”" data-tiara-custom="alex.topType=ì¢‹ì•„ìš”" class="jsx-2157231875 ğŸ¬_selection ğŸ¬_zero_count">
            <div data-animation="SLIDE" class="jsx-2084876625 ğŸ¬_icon"></div>
            <div class="jsx-2157231875 ğŸ¬_selection_label">ìŠ¬í¼ìš”</div>
            <span class="jsx-2157231875 ğŸ¬_count_label">1</span>
        </button>
    </div>
</div>
'''

soup = BeautifulSoup(html, 'html.parser')

# Select the div containing the buttons
reaction_div = soup.find('div', class_='jsx-637502918 ğŸ¬_selections')

# Initialize an empty dictionary to store reaction counts
reaction_counts = {}

# Loop through each button to extract reaction label and count
for button in reaction_div.find_all('button'):
    # Extract the label by finding the div with the specific class
    label = button.find('div', class_='jsx-2157231875 ğŸ¬_selection_label').text.strip()
    # Extract the count by finding the span with the specific class
    count = int(button.find('span', class_='jsx-2157231875 ğŸ¬_count_label').text.strip())
    reaction_counts[label] = count

# Print the reaction counts dictionary
print(reaction_counts)

pd.read_csv('/content/news_data.csv')