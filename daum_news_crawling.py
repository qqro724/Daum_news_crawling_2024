# -*- coding: utf-8 -*-
"""daum_news_crawling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G75DMCoML2f2ni73iMr7AGnuzhpExeXt

###다음 기사 제목 크롤링
"""

import requests
from bs4 import BeautifulSoup

def get_news_content(url):
    response = requests.get(url)
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # 기사 제목 가져오기
        title_tag = soup.find('strong', class_='tit-g')
        if title_tag:
            title = title_tag.get_text(strip=True)
            print(f"기사 제목: {title}")

        # 기사 내용 가져오기
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            print(f"기사 내용: {content}")
    else:
        print("페이지를 가져오는 데 문제가 발생했습니다.")

"""
다음 뉴스 검색 페이지에서, 특정 검색어 검색 결과 50개의
URL 주소, 기사 제목, 기사 내용을 출력
"""
import requests
from bs4 import BeautifulSoup


def daum_search(keyword):
    url = 'https://search.daum.net/search?w=news&DA=PGD&spacing=0'
    # 검색 결과는 1페이지부터 10페이지 까지
    for page in range(1, 11):
        print(f'=== Page {page} ===')
        req_params = {
            'q': keyword, # 검색어(키워드)를 쿼리 스트링에 파라미터로 추가
            'p': page # 검색 페이지 번호를 쿼리 스트링에 파라미터로 추가
        }
        response = requests.get(url, params=req_params)
        html = response.text.strip()

        soup = BeautifulSoup(html, 'html.parser')
        news_links = soup.select('.item-title') # css 선택자대로 .class, #id,
        # tag 와 띄어쓰기로 구분
        for link in news_links:
            news_title = link.text
            news_url = link.get('href')
            print(news_url, news_title)


if __name__ == '__main__':
    daum_search('촉법소년')

"""### 기사 링크 / 기사 제목 /  기사 날짜"""

import requests
from bs4 import BeautifulSoup

def get_news_content(url):
    response = requests.get(url)
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # 기사 제목 가져오기
        title_tag = soup.find('strong', class_='tit-g')
        if title_tag:
            title = title_tag.get_text(strip=True)
            print(f"기사 제목: {title}")

        # 기사 내용 가져오기
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            print(f"기사 내용: {content}")
    else:
        print("페이지를 가져오는 데 문제가 발생했습니다.")

"""
다음 뉴스 검색 페이지에서, 특정 검색어 검색 결과 50개의
URL 주소, 기사 제목, 기사 내용, 기사 날짜, 썸네일 주소를 출력
"""
import requests
from bs4 import BeautifulSoup


def daum_search(keyword):
    url = 'https://search.daum.net/search?w=news&DA=PGD&spacing=0'
    # 검색 결과는 1페이지부터 10페이지 까지
    for page in range(1, 11):
        print(f'=== Page {page} ===')
        req_params = {
            'q': keyword, # 검색어(키워드)를 쿼리 스트링에 파라미터로 추가
            'p': page # 검색 페이지 번호를 쿼리 스트링에 파라미터로 추가
        }
        response = requests.get(url, params=req_params)
        html = response.text.strip()

        soup = BeautifulSoup(html, 'html.parser')
        news_items = soup.select('.item-title') # css 선택자대로 .class, #id,
        # tag 와 띄어쓰기로 구분
        for item in news_items:
            news_title = item.text
            news_url = item.find('a').get('href')
            print(news_url, news_title)

            # 추가 정보 가져오기
            get_additional_info(news_url)


def get_additional_info(url):
    response = requests.get(url)
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # 언론사 가져오기
        press_tag = soup.find('span', class_='info_cp')
        if press_tag:
            press = press_tag.get_text(strip=True)
            print(f"언론사: {press}")

        # 기사 날짜 가져오기
        date_tag = soup.find('span', class_='num_date')
        if date_tag:
            date = date_tag.get_text(strip=True)
            print(f"기사 날짜: {date}")

        # 썸네일 주소 가져오기
        thumbnail_tag = soup.find('img', class_='thumb_g')
        if thumbnail_tag:
            thumbnail_url = thumbnail_tag.get('src')
            print(f"썸네일 주소: {thumbnail_url}")
    else:
        print("페이지를 가져오는 데 문제가 발생했습니다.")


if __name__ == '__main__':
    daum_search('촉법소년')

"""+ ###기사 본문 크롤링"""

import requests
from bs4 import BeautifulSoup

def get_news_content(url):
    response = requests.get(url)
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # 기사 제목 가져오기
        title_tag = soup.find('strong', class_='tit-g')
        if title_tag:
            title = title_tag.get_text(strip=True)
            print(f"기사 제목: {title}")

        # 기사 내용 가져오기
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            print(f"기사 내용: {content}")
    else:
        print("페이지를 가져오는 데 문제가 발생했습니다.")

"""
다음 뉴스 검색 페이지에서, 특정 검색어 검색 결과 50개의
URL 주소, 기사 제목, 기사 내용, 기사 날짜, 썸네일 주소를 출력
"""
import requests
from bs4 import BeautifulSoup


def daum_search(keyword):
    url = 'https://search.daum.net/search?w=news&DA=PGD&spacing=0'
    # 검색 결과는 1페이지부터 10페이지 까지
    for page in range(1, 11):
        print(f'=== Page {page} ===')
        req_params = {
            'q': keyword, # 검색어(키워드)를 쿼리 스트링에 파라미터로 추가
            'p': page # 검색 페이지 번호를 쿼리 스트링에 파라미터로 추가
        }
        response = requests.get(url, params=req_params)
        html = response.text.strip()

        soup = BeautifulSoup(html, 'html.parser')
        news_items = soup.select('.item-title') # css 선택자대로 .class, #id,
        # tag 와 띄어쓰기로 구분
        for item in news_items:
            news_title = item.text
            news_url = item.find('a').get('href')
            print(news_url, news_title)

            # 추가 정보 가져오기
            get_additional_info(news_url)


def get_additional_info(url):
    response = requests.get(url)
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # 언론사 가져오기
        press_tag = soup.find('span', class_='info_cp')
        if press_tag:
            press = press_tag.get_text(strip=True)
            print(f"언론사: {press}")

        # 기사 날짜 가져오기
        date_tag = soup.find('span', class_='num_date')
        if date_tag:
            date = date_tag.get_text(strip=True)
            print(f"기사 날짜: {date}")

        # 썸네일 주소 가져오기
        thumbnail_tag = soup.find('img', class_='thumb_g')
        if thumbnail_tag:
            thumbnail_url = thumbnail_tag.get('src')
            print(f"썸네일 주소: {thumbnail_url}")

        # 기사 본문 내용 가져오기
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            print(f"기사 내용: {content}")
    else:
        print("페이지를 가져오는 데 문제가 발생했습니다.")


if __name__ == '__main__':
    daum_search('촉법소년')

import csv
import requests
from bs4 import BeautifulSoup


def daum_search(keyword):
    url = 'https://search.daum.net/search?w=news&DA=PGD&spacing=0'
    # Create a list to hold all the extracted data
    data_list = []
    # 검색 결과는 1페이지부터 2페이지까지
    for page in range(1, 3):
        print(f'=== 페이지 {page} ===')
        req_params = {
            'q': keyword,  # 검색어(키워드)를 쿼리 스트링에 추가
            'p': page  # 검색 페이지 번호를 쿼리 스트링에 추가
        }
        response = requests.get(url, params=req_params)
        html = response.text.strip()

        soup = BeautifulSoup(html, 'html.parser')
        news_items = soup.select('.item-title')  # CSS 선택자로 .item-title 클래스를 선택
        # 각 뉴스 항목에 대해 처리
        for item in news_items:
            news_title = item.text.strip()
            news_url = item.find('a').get('href')
            print(news_url, news_title)

            # 추가 정보를 추출하고 데이터 리스트에 추가
            additional_info = get_additional_info(news_url)
            data_list.append([news_title, additional_info.get('press', '알 수 없음'), additional_info.get('date', '알 수 없음'), additional_info.get('thumbnail_url', '알 수 없음'), additional_info.get('content', '알 수 없음'), additional_info.get('reviews', {'추천해요': 0, '좋아요': 0, '감동이에요': 0, '화나요': 0, '슬퍼요': 0})])

    # CSV 파일에 데이터 리스트를 씁니다.
    with open('news_data.csv', 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        # 헤더를 작성합니다.
        writer.writerow(['제목', '언론사', '날짜', '썸네일 URL', '내용', '리뷰 수'])
        # 데이터를 씁니다.
        for data_row in data_list:
            # 딕셔너리를 문자열로 변환합니다.
            reviews_str = str(data_row[5])
            # CSV에 데이터를 씁니다.
            writer.writerow(data_row[:5] + [reviews_str])


def get_additional_info(url):
    response = requests.get(url)
    additional_info = {'press': None, 'date': None, 'thumbnail_url': None, 'content': None, 'reviews': None}
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # 언론사 가져오기
        press_tag = soup.find('a', id='kakaoServiceLogo')
        if press_tag:
            additional_info['press'] = press_tag.text.strip()

        # 기사 날짜 가져오기
        date_tag = soup.find('span', class_='num_date')
        if date_tag:
            additional_info['date'] = date_tag.get_text(strip=True)

        # 썸네일 주소 가져오기
        thumbnail_tag = soup.find('a', class_='thumb_bf')
        if thumbnail_tag:
            img_tag = thumbnail_tag.find('img')
            if img_tag:
                additional_info['thumbnail_url'] = img_tag.get('src')

        # 기사 본문 내용 가져오기
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            additional_info['content'] = content

        # 리뷰 가져오기
        reaction_div = soup.find('div', class_='jsx-637502918 🎬_selections')
        reaction_counts = {}
        if reaction_div:
            for button in reaction_div.find_all('button'):
                label = button.find('div', class_='jsx-2157231875 🎬_selection_label').text.strip()
                count = int(button.find('span', class_='jsx-2157231875 🎬_count_label').text.strip())
                reaction_counts[label] = count
        additional_info['reviews'] = reaction_counts
        return additional_info

if __name__ == '__main__':
    daum_search('촉법소년')

"""### 리뷰 가져오기(미완)"""

import csv
import requests
from bs4 import BeautifulSoup


def daum_search(keyword):
    url = 'https://search.daum.net/search?w=news&DA=PGD&spacing=0'
    # Create a list to hold all the extracted data
    data_list = []
    # 검색 결과는 1페이지부터 2페이지까지
    for page in range(1, 3):
        print(f'=== 페이지 {page} ===')
        req_params = {
            'q': keyword,  # 검색어(키워드)를 쿼리 스트링에 추가
            'p': page  # 검색 페이지 번호를 쿼리 스트링에 추가
        }
        response = requests.get(url, params=req_params)
        html = response.text.strip()

        soup = BeautifulSoup(html, 'html.parser')
        news_items = soup.select('.item-title')  # CSS 선택자로 .item-title 클래스를 선택
        # 각 뉴스 항목에 대해 처리
        for item in news_items:
            news_title = item.text.strip()
            news_url = item.find('a').get('href')
            print(news_url, news_title)

            # 추가 정보를 추출하고 데이터 리스트에 추가
            additional_info = get_additional_info(news_url)
            data_list.append([news_title, additional_info.get('press', '알 수 없음'), additional_info.get('date', '알 수 없음'), additional_info.get('thumbnail_url', '알 수 없음'), additional_info.get('content', '알 수 없음'), additional_info.get('reviews', {'추천해요': 0, '좋아요': 0, '감동이에요': 0, '화나요': 0, '슬퍼요': 0})])

    # CSV 파일에 데이터 리스트를 씁니다.
    with open('news_data.csv', 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        # 헤더를 작성합니다.
        writer.writerow(['제목', '언론사', '날짜', '썸네일 URL', '내용', '리뷰 수'])
        # 데이터를 씁니다.
        for data_row in data_list:
            # 딕셔너리를 문자열로 변환합니다.
            reviews_str = str(data_row[5])
            # CSV에 데이터를 씁니다.
            writer.writerow(data_row[:5] + [reviews_str])


def get_additional_info(url):
    response = requests.get(url)
    additional_info = {'press': None, 'date': None, 'thumbnail_url': None, 'content': None, 'reviews': None}
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # 언론사 가져오기
        press_tag = soup.find('a', id='kakaoServiceLogo')
        if press_tag:
            additional_info['press'] = press_tag.text.strip()

        # 기사 날짜 가져오기
        date_tag = soup.find('span', class_='num_date')
        if date_tag:
            additional_info['date'] = date_tag.get_text(strip=True)

        # 썸네일 주소 가져오기
        thumbnail_tag = soup.find('a', class_='thumb_bf')
        if thumbnail_tag:
            img_tag = thumbnail_tag.find('img')
            if img_tag:
                additional_info['thumbnail_url'] = img_tag.get('src')

        # 기사 본문 내용 가져오기
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            additional_info['content'] = content

        # 리뷰 가져오기
        reaction_div = soup.find('div', class_='emotion_list')
        reaction_counts = {}
        if reaction_div:
            review_buttons = reaction_div.find_all('button', class_='jsx-2157231875')
            for button in review_buttons:
                label = button.find('div', class_='jsx-2157231875 🎬_selection_label').text.strip()
                count = button.find('span', class_='jsx-2157231875 🎬_count_label').text.strip()
                reaction_counts[label] = int(count)
            additional_info['reviews'] = reaction_counts if reaction_counts else None  # 리뷰가 없는 경우 None으로 처리

    return additional_info

if __name__ == '__main__':
    daum_search('촉법소년')

import csv
import requests
from bs4 import BeautifulSoup


def daum_search(keyword):
    url = 'https://search.daum.net/search?w=news&DA=PGD&spacing=0'
    # Create a list to hold all the extracted data
    data_list = []
    # 검색 결과는 1페이지부터 2페이지까지
    for page in range(1, 3):
        print(f'=== 페이지 {page} ===')
        req_params = {
            'q': keyword,  # 검색어(키워드)를 쿼리 스트링에 추가
            'p': page  # 검색 페이지 번호를 쿼리 스트링에 추가
        }
        response = requests.get(url, params=req_params)
        html = response.text.strip()

        soup = BeautifulSoup(html, 'html.parser')
        news_items = soup.select('.item-title')  # CSS 선택자로 .item-title 클래스를 선택
        # 각 뉴스 항목에 대해 처리
        for item in news_items:
            news_title = item.text.strip()
            news_url = item.find('a').get('href')
            print(news_url, news_title)

            # 추가 정보를 추출하고 데이터 리스트에 추가
            additional_info = get_additional_info(news_url)
            data_list.append([news_title, additional_info.get('press', '알 수 없음'), additional_info.get('date', '알 수 없음'), additional_info.get('thumbnail_url', '알 수 없음'), additional_info.get('content', '알 수 없음'), additional_info.get('reviews', {'추천해요': 0, '좋아요': 0, '감동이에요': 0, '화나요': 0, '슬퍼요': 0})])

    # CSV 파일에 데이터 리스트를 씁니다.
    with open('news_data.csv', 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        # 헤더를 작성합니다.
        writer.writerow(['제목', '언론사', '날짜', '썸네일 URL', '내용', '리뷰 수'])
        # 데이터를 씁니다.
        for data_row in data_list:
            # 딕셔너리를 문자열로 변환합니다.
            reviews_str = str(data_row[5])
            # CSV에 데이터를 씁니다.
            writer.writerow(data_row[:5] + [reviews_str])


def get_additional_info(url):
    response = requests.get(url)
    additional_info = {'press': None, 'date': None, 'thumbnail_url': None, 'content': None, 'reviews': None}
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # 언론사 가져오기
        press_tag = soup.find('a', id='kakaoServiceLogo')
        if press_tag:
            additional_info['press'] = press_tag.text.strip()

        # 기사 날짜 가져오기
        date_tag = soup.find('span', class_='num_date')
        if date_tag:
            additional_info['date'] = date_tag.get_text(strip=True)

        # 썸네일 주소 가져오기
        thumbnail_tag = soup.find('a', class_='thumb_bf')
        if thumbnail_tag:
            img_tag = thumbnail_tag.find('img')
            if img_tag:
                additional_info['thumbnail_url'] = img_tag.get('src')

        # 기사 본문 내용 가져오기
        content_tag = soup.find('div', class_='article_view')
        if content_tag:
            paragraphs = content_tag.find_all('p', class_=None)
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            additional_info['content'] = content

        # 리뷰 가져오기
        reaction_div = soup.find('div', class_='emotion_list')
        reaction_counts = {}
        if reaction_div:
            review_labels = reaction_div.find_all('div', class_='🎬_selection_label')
            review_counts = reaction_div.find_all('span', class_='🎬_count_label')
            for label, count in zip(review_labels, review_counts):
                reaction_counts[label.text.strip()] = int(count.text.strip())
            additional_info['reviews'] = reaction_counts if reaction_counts else None  # 리뷰가 없는 경우 None으로 처리

    return additional_info

if __name__ == '__main__':
    daum_search('촉법소년')

import pandas as pd

pd.read_csv('/content/news_data.csv')

from bs4 import BeautifulSoup

html = '''
<div data-tiara-custom="alex.component=REACTION&amp;alex.clientId=26BXAvKny5WF5Z09lr5k77Y8&amp;alex.itemKey=20231228152924676&amp;alex.forumKey=news" class="jsx-3762130795 🎬_main">
    <div class="jsx-637502918 🎬_selections">
        <button data-action-type="RECOMMEND" data-tiara-action-name="액션_추천해요" data-tiara-custom="alex.topType=좋아요" class="jsx-2157231875 🎬_selection 🎬_zero_count">
            <div data-animation="SLIDE" class="jsx-2717597395 🎬_icon"></div>
            <div class="jsx-2157231875 🎬_selection_label">추천해요</div>
            <span class="jsx-2157231875 🎬_count_label">3</span>
        </button>
        <button data-action-type="LIKE" data-tiara-action-name="액션_좋아요" data-tiara-custom="alex.topType=좋아요" class="jsx-2157231875 🎬_selection">
            <div data-animation="SLIDE" class="jsx-3429346540 🎬_icon"></div>
            <div class="jsx-2157231875 🎬_selection_label">좋아요</div>
            <span class="jsx-2157231875 🎬_count_label">5</span>
        </button>
        <button data-action-type="IMPRESS" data-tiara-action-name="액션_감동이에요" data-tiara-custom="alex.topType=좋아요" class="jsx-2157231875 🎬_selection 🎬_zero_count">
            <div data-animation="SLIDE" class="jsx-1617167524 🎬_icon"></div>
            <div class="jsx-2157231875 🎬_selection_label">감동이에요</div>
            <span class="jsx-2157231875 🎬_count_label">2</span>
        </button>
        <button data-action-type="ANGRY" data-tiara-action-name="액션_화나요" data-tiara-custom="alex.topType=좋아요" class="jsx-2157231875 🎬_selection 🎬_zero_count">
            <div data-animation="SLIDE" class="jsx-1027214948 🎬_icon"></div>
            <div class="jsx-2157231875 🎬_selection_label">화나요</div>
            <span class="jsx-2157231875 🎬_count_label">0</span>
        </button>
        <button data-action-type="SAD" data-tiara-action-name="액션_슬퍼요" data-tiara-custom="alex.topType=좋아요" class="jsx-2157231875 🎬_selection 🎬_zero_count">
            <div data-animation="SLIDE" class="jsx-2084876625 🎬_icon"></div>
            <div class="jsx-2157231875 🎬_selection_label">슬퍼요</div>
            <span class="jsx-2157231875 🎬_count_label">1</span>
        </button>
    </div>
</div>
'''

soup = BeautifulSoup(html, 'html.parser')

# Select the div containing the buttons
reaction_div = soup.find('div', class_='jsx-637502918 🎬_selections')

# Initialize an empty dictionary to store reaction counts
reaction_counts = {}

# Loop through each button to extract reaction label and count
for button in reaction_div.find_all('button'):
    # Extract the label by finding the div with the specific class
    label = button.find('div', class_='jsx-2157231875 🎬_selection_label').text.strip()
    # Extract the count by finding the span with the specific class
    count = int(button.find('span', class_='jsx-2157231875 🎬_count_label').text.strip())
    reaction_counts[label] = count

# Print the reaction counts dictionary
print(reaction_counts)

pd.read_csv('/content/news_data.csv')